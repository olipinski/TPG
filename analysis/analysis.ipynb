{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "653d34e9633da30e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3ef563b9d1aef6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {},
   "source": [
    "import copy\n",
    "import itertools\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm.notebook import tqdm\n",
    "from emlangkit.language import Language\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "import matplotlib as mpl\n",
    "\n",
    "# Workaround so we can re-use the project functions\n",
    "module_path = os.path.abspath(os.path.join(\"../\"))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from tpg.utils.npmi import (\n",
    "    compute_compositional_ngrams_positionals_npmi,\n",
    "    compute_compositional_ngrams_integers_npmi,\n",
    "    compute_non_compositional_npmi,\n",
    ")\n",
    "from tpg.utils.dict_utils import default_to_regular"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "37e50d00827ae53a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "sns.set(palette=\"pastel\")\n",
    "sns.set_style(\"whitegrid\")\n",
    "palette = sns.color_palette()\n",
    "\n",
    "import matplotlib.pylab as pylab\n",
    "\n",
    "params = {\n",
    "    \"legend.title_fontsize\": \"32\",\n",
    "    \"legend.fontsize\": \"24\",\n",
    "    \"axes.labelsize\": \"32\",\n",
    "    \"axes.titlesize\": \"32\",\n",
    "    \"xtick.labelsize\": \"22\",\n",
    "    \"ytick.labelsize\": \"26\",\n",
    "}\n",
    "pylab.rcParams.update(params)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "205bd118b2157565",
   "metadata": {},
   "source": [
    "## Global defs"
   ]
  },
  {
   "cell_type": "code",
   "id": "6d9dd5cef4871739",
   "metadata": {},
   "source": [
    "top_ns = [1, 2, 3, 5, 10, 15]\n",
    "confidences = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e1d04d31332178d2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "id": "6f46592c4210558a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "all_files = glob.glob(os.path.join(\"./data/logs/\", \"*.json\"))\n",
    "li = []\n",
    "params = []\n",
    "for filename in tqdm(all_files):\n",
    "    split = filename.split(\"-\")\n",
    "    run_id = split[1]\n",
    "    architecture = split[2]\n",
    "    params.append([run_id, architecture])\n",
    "    df = pd.read_json(filename, orient=\"index\")\n",
    "    if run_id != \"test\":\n",
    "        # TODO Comment out the below for full analysis\n",
    "        # df.drop(index=[f\"exchange_{x}\" for x in range(35000)], inplace=True)\n",
    "        pass\n",
    "    for k in [\n",
    "        \"sequence\",\n",
    "        \"cut_inputs\",\n",
    "        \"tds\",\n",
    "        \"message\",\n",
    "        \"guess\",\n",
    "        \"target\",\n",
    "        \"target_id\",\n",
    "    ]:\n",
    "        df[k] = df[k].apply(lambda x: np.array(x))\n",
    "    li.append(df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b2b36eb963ddf4c9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# This code is commented out not to bash WandB servers everytime we run analysis\n",
    "# We run this only once and save to pickle\n",
    "\n",
    "# import wandb\n",
    "#\n",
    "# wandb.login()\n",
    "# api = wandb.Api(timeout=60)\n",
    "#\n",
    "# runs = api.runs(\"user/TPGv5\")\n",
    "# summary_list, config_list, name_list = [], [], []\n",
    "# for run in tqdm(runs):\n",
    "#     summary_list.append(\n",
    "#         run.history(\n",
    "#             samples=400,\n",
    "#         )\n",
    "#     )\n",
    "#\n",
    "#     config_list.append({k: v for k, v in run.config.items()})\n",
    "#\n",
    "#     name_list.append(run.name.split(\"-\")[1])\n",
    "#\n",
    "# runs_full_df = pd.DataFrame(\n",
    "#     {\n",
    "#         \"summary\": summary_list,\n",
    "#         \"config\": config_list,\n",
    "#         \"name\": name_list,\n",
    "#     }\n",
    "# )\n",
    "#\n",
    "# runs_full_df.to_pickle(\"./data/runs_full_df.pickle\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c54ca726385a461a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "runs_full_df = pd.read_pickle(\"./data/runs_full_df_v5.pickle\")\n",
    "runs_full_df = runs_full_df[[\"summary\", \"config\", \"name\"]]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def df_stats(x: pd.DataFrame):\n",
    "    # Some are empty, we'll drop them later\n",
    "    x = x[x[\"val_acc\"].notnull()]\n",
    "    try:\n",
    "        x[\"val_acc\"].idxmax()\n",
    "    except:\n",
    "        return None\n",
    "    max_acc_index = x[\"val_acc\"].idxmax()\n",
    "    over_75_index = x[\"val_acc\"].ge(0.75).any() and x[\"val_acc\"].ge(0.75).idxmax()\n",
    "    over_85_index = x[\"val_acc\"].ge(0.85).any() and x[\"val_acc\"].ge(0.85).idxmax()\n",
    "    end_acc_index = x[\"epoch\"].idxmax()\n",
    "\n",
    "    stats_dict = {\n",
    "        \"max_acc_epoch\": x[\"epoch\"][max_acc_index],\n",
    "        \"max_acc_value\": x[\"val_acc\"][max_acc_index],\n",
    "        \"over_75_epoch\": x[\"epoch\"][over_75_index] if over_75_index else -1,\n",
    "        \"over_75_value\": x[\"val_acc\"][over_75_index] if over_75_index else -1,\n",
    "        \"over_85_epoch\": x[\"epoch\"][over_85_index] if over_85_index else -1,\n",
    "        \"over_85_value\": x[\"val_acc\"][over_85_index] if over_85_index else -1,\n",
    "        \"end_acc_epoch\": x[\"epoch\"][end_acc_index],\n",
    "        \"end_acc_value\": x[\"val_acc\"][end_acc_index],\n",
    "    }\n",
    "\n",
    "    return stats_dict"
   ],
   "id": "7edda73969717838",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "runs_full_df[\"summary\"] = runs_full_df[\"summary\"].apply(df_stats).dropna()\n",
    "df_temp = pd.json_normalize(runs_full_df.pop(\"config\"))\n",
    "runs_full_df = runs_full_df.join(df_temp)\n",
    "df_temp = pd.json_normalize(runs_full_df.pop(\"summary\"))\n",
    "runs_full_df = runs_full_df.join(df_temp)\n",
    "runs_full_df = runs_full_df.set_index(\"name\")"
   ],
   "id": "773e1c77a9b1dfb1",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8d59a20d403f146f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "matches = {\n",
    "    f\"match_{x}\": {\n",
    "        \"run_id\": params[x][0],\n",
    "        \"architecture\": params[x][1],\n",
    "    }\n",
    "    for x in range(len(li))\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "edfb6cb382e34f5b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "for match in tqdm(matches):\n",
    "    run_id = matches[match][\"run_id\"]\n",
    "    if run_id == \"test\":\n",
    "        continue\n",
    "    matches[match][\"max_epochs\"] = runs_full_df.loc[f\"{run_id}\"][\"max_epochs\"].iloc[0]\n",
    "    matches[match][\"dataset_size\"] = runs_full_df.loc[f\"{run_id}\"][\"dataset_size\"].iloc[\n",
    "        0\n",
    "    ]\n",
    "    matches[match][\"num_distractors\"] = runs_full_df.loc[f\"{run_id}\"][\n",
    "        \"num_distractors\"\n",
    "    ].iloc[0]\n",
    "    matches[match][\"seq_length\"] = runs_full_df.loc[f\"{run_id}\"][\"seq_length\"].iloc[0]\n",
    "    matches[match][\"seq_window\"] = runs_full_df.loc[f\"{run_id}\"][\"seq_window\"].iloc[0]\n",
    "    matches[match][\"repeat_chance\"] = runs_full_df.loc[f\"{run_id}\"][\n",
    "        \"repeat_chance\"\n",
    "    ].iloc[0]\n",
    "    matches[match][\"max_length\"] = runs_full_df.loc[f\"{run_id}\"][\"max_length\"].iloc[0]\n",
    "    matches[match][\"vocab_size\"] = runs_full_df.loc[f\"{run_id}\"][\"vocab_size\"].iloc[0]\n",
    "    matches[match][\"one_hot\"] = runs_full_df.loc[f\"{run_id}\"][\"one_hot\"].iloc[0]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c1d0e62527d320a1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "for idx, match in enumerate(matches):\n",
    "    for col in li[idx].columns:\n",
    "        arr = []\n",
    "        for x in li[idx][col]:\n",
    "            arr.append(x)\n",
    "        arr = np.array(arr)\n",
    "        matches[match][col] = arr\n",
    "\n",
    "del li"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "15b06e646be75370",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Test set accuracy"
   ]
  },
  {
   "cell_type": "code",
   "id": "538eba9c021e7526",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "for match in tqdm(matches):\n",
    "    guesses = matches[match][\"guess\"].flatten()\n",
    "    targets = matches[match][\"target_id\"].flatten()\n",
    "    correct = sum(guesses == targets)\n",
    "    total = len(targets)\n",
    "    matches[match][\"test_len\"] = total\n",
    "    matches[match][\"test_acc\"] = correct / total"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2b01c1bf0f5c18b3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Prune unconverged runs"
   ]
  },
  {
   "cell_type": "code",
   "id": "6a183c4efb831954",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Have to pull out the keys, otherwise dict size changes and loop fails\n",
    "mtcs = list(matches.keys())\n",
    "for match in tqdm(mtcs):\n",
    "    if matches[match][\"test_acc\"] < 0.75:\n",
    "        del matches[match]\n",
    "\n",
    "# Re-number matches for later easier processing\n",
    "matches = {f\"match_{i}\": v for i, (k, v) in enumerate(matches.items())}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a1c8918a35bf2e60",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Temporal references across dataset"
   ]
  },
  {
   "cell_type": "code",
   "id": "dfcd0c5a39ba598e",
   "metadata": {},
   "source": [
    "def compute_language_stats(match_to_compute) -> (float, float):\n",
    "    \"\"\"\n",
    "    Compute the language stats for a match.\n",
    "\n",
    "    This uses the emlangkit Langauge function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    match_to_compute: dict\n",
    "        Match for which to compute the stats\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        Computed mpn and mutual information.\n",
    "    \"\"\"\n",
    "    lang = Language(\n",
    "        messages=match_to_compute[\"message\"],\n",
    "        observations=match_to_compute[\"tds\"],\n",
    "        prev_horizon=30,\n",
    "    )\n",
    "    mpn_val = lang.mpn()\n",
    "    mi_val = lang.mutual_information()\n",
    "    return mpn_val, mi_val"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f3e959db-d8f8-4f37-9331-64b8f063348e",
   "metadata": {},
   "source": [
    "start_time = time.perf_counter()\n",
    "\n",
    "results = Parallel(n_jobs=os.cpu_count(), verbose=10)(\n",
    "    delayed(compute_language_stats)(match_to_compute=matches[match])\n",
    "    for match in matches\n",
    ")\n",
    "\n",
    "for x in range(len(matches)):\n",
    "    matches[f\"match_{x}\"][\"mpn_val\"] = copy.deepcopy(results[x][0])\n",
    "    matches[f\"match_{x}\"][\"mi_val\"] = copy.deepcopy(results[x][1])\n",
    "\n",
    "finish_time = time.perf_counter()\n",
    "print(f\"Computing stats finished in {finish_time-start_time} seconds\")\n",
    "del results"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a32761fa286ea157",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Temporal references within dataset"
   ]
  },
  {
   "cell_type": "code",
   "id": "0f070e84-9826-4b18-8929-1230784b20d4",
   "metadata": {},
   "source": [
    "def compute_trwd_stats(match_to_compute) -> (dict, dict):\n",
    "    tpg_dict = defaultdict(\n",
    "        lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(int)))\n",
    "    )\n",
    "    obs_counts_dict = {x: 0 for x in [\"begin\", \"begin+1\", \"end-1\", \"end\"]}\n",
    "    for x in range(match_to_compute[\"test_len\"]):\n",
    "        if \"total\" not in tpg_dict[f'{match_to_compute[\"message\"][x]}']:\n",
    "            tpg_dict[f'{match_to_compute[\"message\"][x]}'][\"total\"] = 0\n",
    "        if \"correct\" not in tpg_dict[f'{match_to_compute[\"message\"][x]}']:\n",
    "            tpg_dict[f'{match_to_compute[\"message\"][x]}'][\"correct\"] = 0\n",
    "        if \"indices\" not in tpg_dict[f'{match_to_compute[\"message\"][x]}']:\n",
    "            tpg_dict[f'{match_to_compute[\"message\"][x]}'][\"indices\"] = []\n",
    "\n",
    "        tpg_dict[f'{match_to_compute[\"message\"][x]}'][\"total\"] += 1\n",
    "        if match_to_compute[\"target_id\"][x][0] == match_to_compute[\"guess\"][x]:\n",
    "            tpg_dict[f'{match_to_compute[\"message\"][x]}'][\"correct\"] += 1\n",
    "\n",
    "        s_obs = match_to_compute[\"cut_inputs\"][x]\n",
    "        if s_obs[0] == -1:\n",
    "            tpg_dict[f'{match_to_compute[\"message\"][x]}'][\"obs_neighbours\"][\"r1\"][\n",
    "                f\"{s_obs[1]}\"\n",
    "            ] += 1\n",
    "            tpg_dict[f'{match_to_compute[\"message\"][x]}'][\"obs_neighbours\"][\"r2\"][\n",
    "                f\"{s_obs[2]}\"\n",
    "            ] += 1\n",
    "            tpg_dict[f'{match_to_compute[\"message\"][x]}'][\"obs_neighbours\"][\"r3\"][\n",
    "                f\"{s_obs[3]}\"\n",
    "            ] += 1\n",
    "            tpg_dict[f'{match_to_compute[\"message\"][x]}'][\"obs_neighbours\"][\"r4\"][\n",
    "                f\"{s_obs[4]}\"\n",
    "            ] += 1\n",
    "\n",
    "            if \"begin\" not in tpg_dict[f'{match_to_compute[\"message\"][x]}']:\n",
    "                tpg_dict[f'{match_to_compute[\"message\"][x]}'][\"begin\"] = 0\n",
    "            tpg_dict[f'{match_to_compute[\"message\"][x]}'][\"begin\"] += 1\n",
    "            obs_counts_dict[\"begin\"] += 1\n",
    "\n",
    "        elif s_obs[1] == -1:\n",
    "            tpg_dict[f'{match_to_compute[\"message\"][x]}'][\"obs_neighbours\"][\"l1\"][\n",
    "                f\"{s_obs[0]}\"\n",
    "            ] += 1\n",
    "            tpg_dict[f'{match_to_compute[\"message\"][x]}'][\"obs_neighbours\"][\"r1\"][\n",
    "                f\"{s_obs[2]}\"\n",
    "            ] += 1\n",
    "            tpg_dict[f'{match_to_compute[\"message\"][x]}'][\"obs_neighbours\"][\"r2\"][\n",
    "                f\"{s_obs[3]}\"\n",
    "            ] += 1\n",
    "            tpg_dict[f'{match_to_compute[\"message\"][x]}'][\"obs_neighbours\"][\"r3\"][\n",
    "                f\"{s_obs[4]}\"\n",
    "            ] += 1\n",
    "\n",
    "            if \"begin+1\" not in tpg_dict[f'{match_to_compute[\"message\"][x]}']:\n",
    "                tpg_dict[f'{match_to_compute[\"message\"][x]}'][\"begin+1\"] = 0\n",
    "            tpg_dict[f'{match_to_compute[\"message\"][x]}'][\"begin+1\"] += 1\n",
    "            obs_counts_dict[\"begin+1\"] += 1\n",
    "\n",
    "        elif s_obs[2] == -1:\n",
    "            tpg_dict[f'{match_to_compute[\"message\"][x]}'][\"obs_neighbours\"][\"l1\"][\n",
    "                f\"{s_obs[1]}\"\n",
    "            ] += 1\n",
    "            tpg_dict[f'{match_to_compute[\"message\"][x]}'][\"obs_neighbours\"][\"l2\"][\n",
    "                f\"{s_obs[0]}\"\n",
    "            ] += 1\n",
    "            tpg_dict[f'{match_to_compute[\"message\"][x]}'][\"obs_neighbours\"][\"r1\"][\n",
    "                f\"{s_obs[3]}\"\n",
    "            ] += 1\n",
    "            tpg_dict[f'{match_to_compute[\"message\"][x]}'][\"obs_neighbours\"][\"r2\"][\n",
    "                f\"{s_obs[4]}\"\n",
    "            ] += 1\n",
    "        elif s_obs[3] == -1:\n",
    "            tpg_dict[f'{match_to_compute[\"message\"][x]}'][\"obs_neighbours\"][\"l1\"][\n",
    "                f\"{s_obs[2]}\"\n",
    "            ] += 1\n",
    "            tpg_dict[f'{match_to_compute[\"message\"][x]}'][\"obs_neighbours\"][\"l2\"][\n",
    "                f\"{s_obs[1]}\"\n",
    "            ] += 1\n",
    "            tpg_dict[f'{match_to_compute[\"message\"][x]}'][\"obs_neighbours\"][\"l3\"][\n",
    "                f\"{s_obs[0]}\"\n",
    "            ] += 1\n",
    "            tpg_dict[f'{match_to_compute[\"message\"][x]}'][\"obs_neighbours\"][\"r1\"][\n",
    "                f\"{s_obs[4]}\"\n",
    "            ] += 1\n",
    "            if \"end-1\" not in tpg_dict[f'{match_to_compute[\"message\"][x]}']:\n",
    "                tpg_dict[f'{match_to_compute[\"message\"][x]}'][\"end-1\"] = 0\n",
    "            tpg_dict[f'{match_to_compute[\"message\"][x]}'][\"end-1\"] += 1\n",
    "            obs_counts_dict[\"end-1\"] += 1\n",
    "        elif s_obs[4] == -1:\n",
    "            tpg_dict[f'{match_to_compute[\"message\"][x]}'][\"obs_neighbours\"][\"l1\"][\n",
    "                f\"{s_obs[3]}\"\n",
    "            ] += 1\n",
    "            tpg_dict[f'{match_to_compute[\"message\"][x]}'][\"obs_neighbours\"][\"l2\"][\n",
    "                f\"{s_obs[2]}\"\n",
    "            ] += 1\n",
    "            tpg_dict[f'{match_to_compute[\"message\"][x]}'][\"obs_neighbours\"][\"l3\"][\n",
    "                f\"{s_obs[1]}\"\n",
    "            ] += 1\n",
    "            tpg_dict[f'{match_to_compute[\"message\"][x]}'][\"obs_neighbours\"][\"l4\"][\n",
    "                f\"{s_obs[0]}\"\n",
    "            ] += 1\n",
    "            if \"end\" not in tpg_dict[f'{match_to_compute[\"message\"][x]}']:\n",
    "                tpg_dict[f'{match_to_compute[\"message\"][x]}'][\"end\"] = 0\n",
    "            tpg_dict[f'{match_to_compute[\"message\"][x]}'][\"end\"] += 1\n",
    "            obs_counts_dict[\"end\"] += 1\n",
    "\n",
    "    for key, v in tpg_dict.items():\n",
    "        values, counts = np.unique(\n",
    "            np.array(tpg_dict[key][\"indices\"]), return_counts=True\n",
    "        )\n",
    "        tpg_dict[key][\"indices_unq\"] = {\n",
    "            value: countt for value, countt in zip(values, counts)\n",
    "        }\n",
    "    return tpg_dict, obs_counts_dict"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "452c33df63bc70e9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "start_time = time.perf_counter()\n",
    "\n",
    "results = Parallel(n_jobs=os.cpu_count(), verbose=10)(\n",
    "    delayed(compute_trwd_stats)(match_to_compute=matches[match]) for match in matches\n",
    ")\n",
    "\n",
    "for x in range(len(matches)):\n",
    "    matches[f\"match_{x}\"][\"tpg_stats\"] = copy.deepcopy(results[x][0])\n",
    "    matches[f\"match_{x}\"][\"obs_counts\"] = copy.deepcopy(results[x][1])\n",
    "\n",
    "finish_time = time.perf_counter()\n",
    "print(f\"Computing stats finished in {finish_time-start_time} seconds\")\n",
    "del results"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7fedad9b41ae7fac",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Non-compositional NPMI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0e66c5fa7be3ff",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Here we calculate the NPMI for non-compositional messages, including those which are used as special operators, i.e., \"begin\", \"begin+1\", \"end-1\" or \"end\"."
   ]
  },
  {
   "cell_type": "code",
   "id": "18474dc5b19f9376",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Calculate the normalised pointwise mutual information for non-compositional messages\n",
    "# There is some divisions by nans, so we ignore this for this block\n",
    "np.seterr(divide=\"ignore\", invalid=\"ignore\")\n",
    "\n",
    "# Check for the top_n integer/pos combinations\n",
    "for top_n in top_ns:\n",
    "    for match in tqdm(matches):\n",
    "        non_compositional_npmi_dict = compute_non_compositional_npmi(\n",
    "            matches[match], top_n\n",
    "        )\n",
    "        matches[match][f\"nc_npmi_{top_n}\"] = non_compositional_npmi_dict\n",
    "\n",
    "np.seterr(divide=\"warn\", invalid=\"warn\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "198fcba81ec507fc",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Compositional (n-grams) NPMI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad242320cd15ea9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Here, we assume some compositionality, and test whether specific ngrams refer to position, together with another n-gram referring to the integer in that position"
   ]
  },
  {
   "cell_type": "code",
   "id": "6af97ac596481fe0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Generate all n-grams\n",
    "# We do it the easy way-ish by generating all messages with say starting 1 and checking them all\n",
    "# \"16 20\" in \"[1 16 20]\"\n",
    "n_grams = defaultdict(dict)\n",
    "for x in [1, 2, 3]:\n",
    "    for n_gram in list(itertools.product([x for x in range(26)], repeat=x)):\n",
    "        n_grams[n_gram][\"length\"] = x\n",
    "n_grams = {\n",
    "    str(n_gram)\n",
    "    .replace(\"(\", \"\")\n",
    "    .replace(\")\", \"\")\n",
    "    .replace(\",\", \"\"): n_grams[n_gram][\"length\"]\n",
    "    for n_gram in n_grams.keys()\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "179462a8e1a79bb3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "start_time = time.perf_counter()\n",
    "\n",
    "for top_n in top_ns:\n",
    "    results = Parallel(n_jobs=os.cpu_count(), verbose=10)(\n",
    "        delayed(compute_compositional_ngrams_integers_npmi)(\n",
    "            match=matches[match], n_grams=n_grams, top_n=top_n\n",
    "        )\n",
    "        for match in matches\n",
    "    )\n",
    "\n",
    "    for x in range(len(matches)):\n",
    "        matches[f\"match_{x}\"][f\"ngram_npmi_integers_{top_n}\"] = copy.deepcopy(\n",
    "            results[x][0]\n",
    "        )\n",
    "        if \"ngrams_pruned\" not in matches[f\"match_{x}\"]:\n",
    "            matches[f\"match_{x}\"][\"ngrams_pruned\"] = copy.deepcopy(results[x][1])\n",
    "\n",
    "finish_time = time.perf_counter()\n",
    "print(f\"Computing stats finished in {finish_time-start_time} seconds\")\n",
    "del results"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4c7919e1bb97a650",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "start_time = time.perf_counter()\n",
    "\n",
    "for confidence in confidences:\n",
    "    for top_n in top_ns:\n",
    "        results = Parallel(n_jobs=os.cpu_count(), verbose=10)(\n",
    "            delayed(compute_compositional_ngrams_positionals_npmi)(\n",
    "                match=matches[match],\n",
    "                n_grams=n_grams,\n",
    "                confidence=confidence,\n",
    "                top_n=top_n,\n",
    "                scale=10,\n",
    "            )\n",
    "            for match in matches\n",
    "        )\n",
    "\n",
    "        for x in range(len(matches)):\n",
    "            matches[f\"match_{x}\"][\n",
    "                f\"ngram_npmi_positionals_{top_n}_{confidence}\"\n",
    "            ] = copy.deepcopy(results[x])\n",
    "\n",
    "finish_time = time.perf_counter()\n",
    "print(f\"Computing metrics finished in {finish_time-start_time} seconds\")\n",
    "del results"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ffacc0c5adf4d662",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Visualising data"
   ]
  },
  {
   "cell_type": "code",
   "id": "f50692493bef19c2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "for match in matches:\n",
    "    matches[match][\"non_compositional_emerged\"] = set()\n",
    "    matches[match][\"non_compositional_reserved_emerged\"] = set()\n",
    "    matches[match][\"non_compositional_int_emerged\"] = set()\n",
    "    matches[match][\"compositional_pos_emerged\"] = set()\n",
    "    matches[match][\"compositional_int_emerged\"] = set()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "19c4c9001a09d357",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Find all the messages that are non-compositional\n",
    "nc_dicts = {}\n",
    "\n",
    "for top_n in tqdm(top_ns):\n",
    "    for confidence in confidences:\n",
    "        non_compositional_message_translation_dict = {}\n",
    "        for match in matches:\n",
    "            non_compositional_identified = []\n",
    "            non_compositional_message_translation_dict[match] = {\n",
    "                \"arch\": matches[match][\"architecture\"],\n",
    "                \"run_id\": matches[match][\"run_id\"],\n",
    "                \"positional_messages\": {\n",
    "                    x: [] for x in [\"begin\", \"begin+1\", \"end-1\", \"end\"]\n",
    "                },\n",
    "                \"other_messages\": defaultdict(lambda: defaultdict(list)),\n",
    "            }\n",
    "            non_compositional_npmi_dict = matches[match][f\"nc_npmi_{top_n}\"]\n",
    "            for msg in non_compositional_npmi_dict:\n",
    "                for special in [\"begin\", \"begin+1\", \"end-1\", \"end\"]:\n",
    "                    if non_compositional_npmi_dict[msg][special] >= confidence:\n",
    "                        # print(f\"{msg} is {special} in {match}\")\n",
    "                        non_compositional_identified.append(msg)\n",
    "                        non_compositional_message_translation_dict[match][\n",
    "                            \"positional_messages\"\n",
    "                        ][special].append(\n",
    "                            np.fromstring(\n",
    "                                msg.replace(\"[\", \"\").replace(\"]\", \"\").strip(),\n",
    "                                sep=\" \",\n",
    "                                dtype=np.int8,\n",
    "                            )\n",
    "                        )\n",
    "                        matches[match][\"non_compositional_emerged\"].add(\n",
    "                            f\"{top_n}_{confidence}\"\n",
    "                        )\n",
    "                for pos in non_compositional_npmi_dict[msg]:\n",
    "                    if pos in [\"begin\", \"begin+1\", \"end-1\", \"end\"]:\n",
    "                        continue\n",
    "                    if non_compositional_npmi_dict[msg][pos][\"npmi\"] >= confidence:\n",
    "                        ints = [\n",
    "                            int(x)\n",
    "                            for x in non_compositional_npmi_dict[msg][pos][\"ints\"]\n",
    "                        ]\n",
    "                        # print(f\"{msg} is {pos} for {ints} in {match}\")\n",
    "                        for x in ints:\n",
    "                            non_compositional_message_translation_dict[match][\n",
    "                                \"other_messages\"\n",
    "                            ][pos][x].append(\n",
    "                                np.fromstring(\n",
    "                                    msg.replace(\"[\", \"\").replace(\"]\", \"\").strip(),\n",
    "                                    sep=\" \",\n",
    "                                    dtype=np.int8,\n",
    "                                )\n",
    "                            )\n",
    "                        matches[match][\"non_compositional_int_emerged\"].add(\n",
    "                            f\"{top_n}_{confidence}\"\n",
    "                        )\n",
    "\n",
    "            for msg in non_compositional_identified:\n",
    "                count = 0\n",
    "                msg_c = [\n",
    "                    x\n",
    "                    for x in msg.replace(\"[\", \"\").replace(\"]\", \"\").strip().split(\" \")\n",
    "                    if x\n",
    "                ]\n",
    "\n",
    "                if msg_c[0] == msg_c[1] == msg_c[2]:\n",
    "                    if len(msg_c[0]) == 1:\n",
    "                        msg_c[0] = msg_c[0].join(\n",
    "                            \" \"\n",
    "                        )  # Make sure 1 is present as 1 not as 11, for example\n",
    "                    for msg1 in matches[match][\"tpg_stats\"]:\n",
    "                        if msg_c[0] in msg1:\n",
    "                            count += 1\n",
    "                else:\n",
    "                    continue\n",
    "                if count <= 2:\n",
    "                    matches[match][\"non_compositional_reserved_emerged\"].add(\n",
    "                        f\"{top_n}_{confidence}\"\n",
    "                    )\n",
    "\n",
    "        nc_dicts[\n",
    "            f\"topn_{top_n}-confidence_{confidence}\"\n",
    "        ] = non_compositional_message_translation_dict"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bf69658aacd25919",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Find all n-grams that may represent some integers\n",
    "c_dicts = {}\n",
    "for top_n in tqdm(top_ns):\n",
    "    for confidence in confidences:\n",
    "        compositional_message_translation_dict = {}\n",
    "        for match in matches:\n",
    "            compositional_message_translation_dict[match] = {\n",
    "                \"arch\": matches[match][\"architecture\"],\n",
    "                \"run_id\": matches[match][\"run_id\"],\n",
    "                \"positional_ngrams\": defaultdict(\n",
    "                    lambda: defaultdict(list)\n",
    "                ),  # format is {requested_pos_reference: {needed_pos: [ngrams]}}\n",
    "                \"integer_ngrams\": defaultdict(\n",
    "                    lambda: defaultdict(list)\n",
    "                ),  # format is {requested_int_reference: {needed_pos: [ngrams]}}\n",
    "            }\n",
    "            ngram_npmi_integers_dict = matches[match][f\"ngram_npmi_integers_{top_n}\"]\n",
    "            ngram_npmi_positionals_dict = matches[match][\n",
    "                f\"ngram_npmi_positionals_{top_n}_{confidence}\"\n",
    "            ]\n",
    "\n",
    "            if len(ngram_npmi_positionals_dict.keys()) > 1:\n",
    "                matches[match][\"compositional_pos_emerged\"].add(f\"{top_n}_{confidence}\")\n",
    "\n",
    "            if len(ngram_npmi_integers_dict.keys()) > 1:\n",
    "                matches[match][\"compositional_int_emerged\"].add(f\"{top_n}_{confidence}\")\n",
    "\n",
    "            for ngram in ngram_npmi_integers_dict:\n",
    "                ngram_np = np.array([x for x in ngram.split(\" \") if x], dtype=np.uint8)\n",
    "                for pos in ngram_npmi_integers_dict[ngram]:\n",
    "                    if len(ngram_npmi_integers_dict[ngram][pos]) == 0:\n",
    "                        continue\n",
    "                    if ngram_npmi_integers_dict[ngram][pos][\"value\"] > confidence:\n",
    "                        for x in ngram_npmi_integers_dict[ngram][pos][\"integers\"]:\n",
    "                            compositional_message_translation_dict[match][\n",
    "                                \"integer_ngrams\"\n",
    "                            ][pos][int(x)].append(ngram_np)\n",
    "            for ngram in ngram_npmi_positionals_dict:\n",
    "                ngram_np = np.array([x for x in ngram.split(\" \")], dtype=np.uint8)\n",
    "                for pos in ngram_npmi_positionals_dict[ngram]:\n",
    "                    if len(ngram_npmi_positionals_dict[ngram][pos]) == 0:\n",
    "                        continue\n",
    "                    for referent_pos in ngram_npmi_positionals_dict[ngram][pos]:\n",
    "                        if (\n",
    "                            ngram_npmi_positionals_dict[ngram][pos][referent_pos]\n",
    "                            > confidence\n",
    "                        ):\n",
    "                            compositional_message_translation_dict[match][\n",
    "                                \"positional_ngrams\"\n",
    "                            ][pos][referent_pos].append(ngram_np)\n",
    "        c_dicts[\n",
    "            f\"topn_{top_n}-confidence_{confidence}\"\n",
    "        ] = compositional_message_translation_dict"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3192436fdf5dda52",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Find all runs where temporal references have emerged\n",
    "for match in matches:\n",
    "    if any(matches[match][\"mpn_val\"] > 99):\n",
    "        matches[match][\"mpn_emerged\"] = 1"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "52601090c8340c64",
   "metadata": {},
   "source": [
    "### Save the data\n",
    "This data is needed to run the evaluations."
   ]
  },
  {
   "cell_type": "code",
   "id": "c9c59834d02df968",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "matches_dict = default_to_regular(matches)\n",
    "with open(\"matches.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(matches_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "69f256ae558459a0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "nc_dicts = default_to_regular(nc_dicts)\n",
    "with open(\"dictionary_nc.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(\n",
    "        nc_dicts,\n",
    "        handle,\n",
    "        protocol=pickle.HIGHEST_PROTOCOL,\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "415af51f4dcffcce",
   "metadata": {},
   "source": [
    "c_dicts = default_to_regular(c_dicts)\n",
    "with open(\"dictionary_c.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(\n",
    "        c_dicts,\n",
    "        handle,\n",
    "        protocol=pickle.HIGHEST_PROTOCOL,\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f25a9051a6ebf881",
   "metadata": {},
   "source": [
    "### Reload the data"
   ]
  },
  {
   "cell_type": "code",
   "id": "3f64f671c9d48334",
   "metadata": {},
   "source": [
    "with open(\"matches.pickle\", \"rb\") as handle:\n",
    "    matches = pickle.load(handle)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bdebcdce1e12c45",
   "metadata": {},
   "source": [
    "with open(\"dictionary_nc.pickle\", \"rb\") as handle:\n",
    "    nc_dicts = pickle.load(handle)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7f7c414173614fb3",
   "metadata": {},
   "source": [
    "with open(\"dictionary_c.pickle\", \"rb\") as handle:\n",
    "    c_dicts = pickle.load(handle)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "47a11e90f6e93857",
   "metadata": {},
   "source": [
    "### Load the Accuracy Data\n",
    "This data comes from the evaluation. So to run the evaluation, the dictionaries must be saved above, the evaluation ran, and then we come back to this point."
   ]
  },
  {
   "cell_type": "code",
   "id": "8ea86abb1da5e12d",
   "metadata": {},
   "source": [
    "with open(\"agent_accuracy_full.pickle\", \"rb\") as handle:\n",
    "    agent_accuracy_full = pickle.load(handle)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9625d0173a1a9f7c",
   "metadata": {},
   "source": [
    "with open(\"agent_dict_lens_full.pickle\", \"rb\") as handle:\n",
    "    agent_dict_lens_full = pickle.load(handle)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fa4899391e99c8de",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "id": "10719be23666d8c8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "emergence_dict = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))\n",
    "for top_n in tqdm(top_ns):\n",
    "    for confidence in confidences:\n",
    "        for match in matches:\n",
    "            # We skip the other architectures\n",
    "            if \"GRU\" not in matches[match][\"architecture\"]:\n",
    "                continue\n",
    "            emergence_dict[matches[match][\"architecture\"]][\"total\"][\n",
    "                f\"{top_n}_{confidence}\"\n",
    "            ] += 1\n",
    "            emergence_dict[matches[match][\"architecture\"]][\"accuracy\"][\n",
    "                f\"{top_n}_{confidence}\"\n",
    "            ] += matches[match][\"test_acc\"]\n",
    "            emergence_dict[matches[match][\"architecture\"]][\"non_compositional_emerged\"][\n",
    "                f\"{top_n}_{confidence}\"\n",
    "            ] += (\n",
    "                1\n",
    "                if f\"{top_n}_{confidence}\"\n",
    "                in matches[match][\"non_compositional_emerged\"]\n",
    "                else 0\n",
    "            )\n",
    "            emergence_dict[matches[match][\"architecture\"]][\n",
    "                \"non_compositional_int_emerged\"\n",
    "            ][f\"{top_n}_{confidence}\"] += (\n",
    "                1\n",
    "                if f\"{top_n}_{confidence}\"\n",
    "                in matches[match][\"non_compositional_int_emerged\"]\n",
    "                else 0\n",
    "            )\n",
    "            emergence_dict[matches[match][\"architecture\"]][\n",
    "                \"non_compositional_reserved_emerged\"\n",
    "            ][f\"{top_n}_{confidence}\"] += (\n",
    "                1\n",
    "                if f\"{top_n}_{confidence}\"\n",
    "                in matches[match][\"non_compositional_reserved_emerged\"]\n",
    "                else 0\n",
    "            )\n",
    "            emergence_dict[matches[match][\"architecture\"]][\"compositional_int_emerged\"][\n",
    "                f\"{top_n}_{confidence}\"\n",
    "            ] += (\n",
    "                1\n",
    "                if f\"{top_n}_{confidence}\"\n",
    "                in matches[match][\"compositional_int_emerged\"]\n",
    "                else 0\n",
    "            )\n",
    "            emergence_dict[matches[match][\"architecture\"]][\"compositional_pos_emerged\"][\n",
    "                f\"{top_n}_{confidence}\"\n",
    "            ] += (\n",
    "                1\n",
    "                if f\"{top_n}_{confidence}\"\n",
    "                in matches[match][\"compositional_pos_emerged\"]\n",
    "                else 0\n",
    "            )\n",
    "\n",
    "for top_n in tqdm(top_ns):\n",
    "    for confidence in confidences:\n",
    "        for k in emergence_dict:\n",
    "            emergence_dict[k][\"accuracy\"][f\"{top_n}_{confidence}\"] /= emergence_dict[k][\n",
    "                \"total\"\n",
    "            ][f\"{top_n}_{confidence}\"]\n",
    "            emergence_dict[k][\"non_compositional_emerged\"][\n",
    "                f\"{top_n}_{confidence}\"\n",
    "            ] /= emergence_dict[k][\"total\"][f\"{top_n}_{confidence}\"]\n",
    "            emergence_dict[k][\"non_compositional_int_emerged\"][\n",
    "                f\"{top_n}_{confidence}\"\n",
    "            ] /= emergence_dict[k][\"total\"][f\"{top_n}_{confidence}\"]\n",
    "            emergence_dict[k][\"non_compositional_reserved_emerged\"][\n",
    "                f\"{top_n}_{confidence}\"\n",
    "            ] /= emergence_dict[k][\"total\"][f\"{top_n}_{confidence}\"]\n",
    "            emergence_dict[k][\"compositional_int_emerged\"][\n",
    "                f\"{top_n}_{confidence}\"\n",
    "            ] /= emergence_dict[k][\"total\"][f\"{top_n}_{confidence}\"]\n",
    "            emergence_dict[k][\"compositional_pos_emerged\"][\n",
    "                f\"{top_n}_{confidence}\"\n",
    "            ] /= emergence_dict[k][\"total\"][f\"{top_n}_{confidence}\"]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "dd76a5cce7ae514a",
   "metadata": {},
   "source": [
    "for k in emergence_dict[\"BaseGRU\"]:\n",
    "    total = 0\n",
    "    for ks in emergence_dict[\"BaseGRU\"][k]:\n",
    "        if \"1_\" in ks:\n",
    "            total += emergence_dict[\"BaseGRU\"][k][ks]\n",
    "    print(f\"Average {k} is {total/9}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f32105bc39d8bce1",
   "metadata": {},
   "source": [
    "## Vocabulary coverage"
   ]
  },
  {
   "cell_type": "code",
   "id": "69384f50a795ff93",
   "metadata": {},
   "source": [
    "coverages_p_full = []\n",
    "coverages_nc_full = []\n",
    "\n",
    "for confidence in confidences:\n",
    "    for top_n in top_ns:\n",
    "        coverages_p = []\n",
    "        coverages_nc = []\n",
    "        coverages_c = []\n",
    "        coverages_total = []\n",
    "\n",
    "        for match in matches:\n",
    "            # We are only interested in BaseGRU\n",
    "            if matches[match][\"architecture\"] != \"BaseGRU\":\n",
    "                continue\n",
    "            msg_set = set(list(matches[match][\"tpg_stats\"].keys()))\n",
    "\n",
    "            # Non-compositional messages are quite straightforward\n",
    "            identified_msgs_nc_p = list(\n",
    "                nc_dicts[f\"topn_{top_n}-confidence_{confidence}\"][match][\n",
    "                    \"positional_messages\"\n",
    "                ].values()\n",
    "            )\n",
    "            identified_msgs_nc_o = list(\n",
    "                nc_dicts[f\"topn_{top_n}-confidence_{confidence}\"][match][\n",
    "                    \"other_messages\"\n",
    "                ].values()\n",
    "            )\n",
    "            count_nc_p = len(list(itertools.chain(*identified_msgs_nc_p)))\n",
    "            count_nc_o = len(list(itertools.chain(*identified_msgs_nc_o)))\n",
    "            coverages_p.append(count_nc_p / len(msg_set))\n",
    "            coverages_nc.append(count_nc_o / len(msg_set))\n",
    "\n",
    "        coverages_p_full.append(\n",
    "            {\n",
    "                \"confidence\": confidence,\n",
    "                \"mean\": np.mean(coverages_p),\n",
    "            }  # There is no top_n in positional messages\n",
    "        )\n",
    "        coverages_nc_full.append(\n",
    "            {\"top_n\": top_n, \"confidence\": confidence, \"mean\": np.mean(coverages_nc)}\n",
    "        )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "936666e2b175a3f8",
   "metadata": {},
   "source": [
    "def compute_compo_stats(\n",
    "    matchh, match_int_c_dict, match_pos_c_dict, top_nn, confidencee\n",
    "):\n",
    "    coverages_compo_pos = []\n",
    "    coverages_compo_int = []\n",
    "    int_ngrams_present = 0\n",
    "    pos_ngrams_present = 0\n",
    "\n",
    "    messages = matchh[\"message\"]\n",
    "    obs = matchh[\"cut_inputs\"]\n",
    "\n",
    "    pos_int_dict = {\n",
    "        int_pos: list(match_int_c_dict[int_pos].keys())\n",
    "        for int_pos in [\"npmi_pos_0\", \"npmi_pos_1\", \"npmi_pos_2\", \"inv_npmi\"]\n",
    "    }\n",
    "\n",
    "    pos_pos_dict = {\n",
    "        pos_pos: list(match_pos_c_dict[pos_pos].keys())\n",
    "        for pos_pos in [\"npmi_pos_0\", \"npmi_pos_1\", \"npmi_pos_2\", \"inv_npmi\"]\n",
    "    }\n",
    "\n",
    "    # Check if integer n-grams possibly used\n",
    "    for msg_id, (msgg, observation) in enumerate(zip(messages, obs)):\n",
    "        msgg = list(msgg)\n",
    "        int_ngram_found = False\n",
    "        pos_ngram_found = False\n",
    "\n",
    "        int_poses = []\n",
    "        for pos in [\"npmi_pos_0\", \"npmi_pos_1\", \"npmi_pos_2\", \"inv_npmi\"]:\n",
    "            valid_ints_pos = pos_int_dict[pos]\n",
    "            # valid_ints = [valid_int for valid_int in valid_ints if valid_int in observation]\n",
    "            valid_ints = []\n",
    "            # Less generalisable, but so much faster\n",
    "            for xx in range(5):\n",
    "                if observation[xx] in valid_ints_pos:\n",
    "                    valid_ints.append(observation[xx])\n",
    "            if any(valid_ints):\n",
    "                for valid_int in valid_ints:\n",
    "                    for ngram in match_int_c_dict[pos][valid_int]:\n",
    "                        ngram = list(ngram)\n",
    "                        if \"0\" in pos:\n",
    "                            start_pos = 0\n",
    "                        elif \"1\" in pos:\n",
    "                            start_pos = 1\n",
    "                        elif \"2\" in pos:\n",
    "                            start_pos = 2\n",
    "                        elif \"inv\" in pos:\n",
    "                            start_pos = -1\n",
    "                        else:\n",
    "                            raise ValueError(\"Invalid pos.\")\n",
    "\n",
    "                        if start_pos != -1:\n",
    "                            if ngram == msgg[start_pos : start_pos + len(ngram)]:\n",
    "                                int_ngram_found = True\n",
    "                                int_poses.append(\n",
    "                                    {\n",
    "                                        \"ngram\": ngram,\n",
    "                                        \"msg\": msgg,\n",
    "                                        \"length\": len(ngram),\n",
    "                                        \"pos\": pos,\n",
    "                                        \"integer\": valid_int,\n",
    "                                    }\n",
    "                                )\n",
    "                        else:\n",
    "                            # Check all possible positions for the invariant ngrams\n",
    "                            for start_pos in range(4 - len(ngram)):\n",
    "                                if ngram == msgg[start_pos : start_pos + len(ngram)]:\n",
    "                                    int_ngram_found = True\n",
    "                                    int_poses.append(\n",
    "                                        {\n",
    "                                            \"ngram\": ngram,\n",
    "                                            \"msg\": msgg,\n",
    "                                            \"length\": len(ngram),\n",
    "                                            \"pos\": pos,\n",
    "                                            \"integer\": valid_int,\n",
    "                                        }\n",
    "                                    )\n",
    "\n",
    "        relative_pos_integers = {\n",
    "            \"l\": [],\n",
    "            \"r\": [],\n",
    "        }\n",
    "        target_id = np.where(observation == -1)[0][0]\n",
    "        for msg_id in range(1, 5):\n",
    "            # Traverse left\n",
    "            if target_id - msg_id >= 0:\n",
    "                relative_pos_integers[f\"l{msg_id}\"] = observation[\n",
    "                    target_id - msg_id\n",
    "                ].item()\n",
    "                relative_pos_integers[\"l\"].append(\n",
    "                    observation[target_id - msg_id].item()\n",
    "                )\n",
    "            # Traverse right\n",
    "            if target_id + msg_id < len(observation):\n",
    "                relative_pos_integers[f\"r{msg_id}\"] = observation[\n",
    "                    target_id + msg_id\n",
    "                ].item()\n",
    "                relative_pos_integers[\"r\"].append(\n",
    "                    observation[target_id + msg_id].item()\n",
    "                )\n",
    "\n",
    "        valid_obs_pos = [\n",
    "            k\n",
    "            for k in relative_pos_integers.keys()\n",
    "            if relative_pos_integers[k] != -1 and relative_pos_integers[k] != []\n",
    "        ]\n",
    "\n",
    "        pos_poses = []\n",
    "        if f\"{top_nn}_{confidencee}\" in matchh[\"compositional_pos_emerged\"]:\n",
    "            for pos in [\"npmi_pos_0\", \"npmi_pos_1\", \"npmi_pos_2\", \"inv_npmi\"]:\n",
    "                valid_poss = pos_pos_dict[pos]\n",
    "                valid_poss = [x for x in valid_poss if x in valid_obs_pos]\n",
    "                for obs_pos in valid_poss:\n",
    "                    for ngram in match_pos_c_dict[pos][obs_pos]:\n",
    "                        ngram = list(ngram)\n",
    "                        if \"0\" in pos:\n",
    "                            start_pos = 0\n",
    "                        elif \"1\" in pos:\n",
    "                            start_pos = 1\n",
    "                        elif \"2\" in pos:\n",
    "                            start_pos = 2\n",
    "                        elif \"inv\" in pos:\n",
    "                            start_pos = -1\n",
    "                        else:\n",
    "                            raise ValueError(\"Invalid pos.\")\n",
    "\n",
    "                        if start_pos != -1:\n",
    "                            if ngram == msgg[start_pos : start_pos + len(ngram)]:\n",
    "                                pos_ngram_found = True\n",
    "                                pos_poses.append(\n",
    "                                    {\n",
    "                                        \"ngram\": ngram,\n",
    "                                        \"msg\": msgg,\n",
    "                                        \"length\": len(ngram),\n",
    "                                        \"pos\": pos,\n",
    "                                        \"ref_pos\": obs_pos,\n",
    "                                    }\n",
    "                                )\n",
    "                        else:\n",
    "                            # Check all possible positions for the invariant ngrams\n",
    "                            for start_pos in range(4 - len(ngram)):\n",
    "                                if ngram == msgg[start_pos : start_pos + len(ngram)]:\n",
    "                                    pos_ngram_found = True\n",
    "                                    pos_poses.append(\n",
    "                                        {\n",
    "                                            \"ngram\": ngram,\n",
    "                                            \"msg\": msgg,\n",
    "                                            \"length\": len(ngram),\n",
    "                                            \"pos\": pos,\n",
    "                                            \"ref_pos\": obs_pos,\n",
    "                                        }\n",
    "                                    )\n",
    "\n",
    "        int_ngrams_present += int_ngram_found\n",
    "        pos_ngrams_present += pos_ngram_found\n",
    "\n",
    "    count_cc = len(matchh[\"message\"])\n",
    "    coverages_compo_int.append(int_ngrams_present / count_cc)\n",
    "    if f\"{top_nn}_{confidencee}\" in matchh[\"compositional_pos_emerged\"]:\n",
    "        coverages_compo_pos.append(pos_ngrams_present / count_cc)\n",
    "\n",
    "    return (\n",
    "        coverages_compo_int,\n",
    "        coverages_compo_pos,\n",
    "        confidencee,\n",
    "        top_nn,\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "94b9514e-9596-4f9b-8072-003d14e893b3",
   "metadata": {},
   "source": [
    "coverages_compo_pos_full = []\n",
    "coverages_compo_int_full = []\n",
    "\n",
    "results = Parallel(n_jobs=os.cpu_count() * 2, verbose=10)(\n",
    "    delayed(compute_compo_stats)(\n",
    "        matchh=matches[match],\n",
    "        confidencee=confidence,\n",
    "        top_nn=top_n,\n",
    "        match_int_c_dict=c_dicts[f\"topn_{top_n}-confidence_{confidence}\"][match][\n",
    "            \"integer_ngrams\"\n",
    "        ],\n",
    "        match_pos_c_dict=c_dicts[f\"topn_{top_n}-confidence_{confidence}\"][match][\n",
    "            \"positional_ngrams\"\n",
    "        ],\n",
    "    )\n",
    "    for match in matches\n",
    "    if matches[match][\"architecture\"] == \"BaseGRU\"\n",
    "    for confidence in confidences\n",
    "    for top_n in top_ns\n",
    ")\n",
    "\n",
    "for res in results:\n",
    "    res_int = res[0]\n",
    "    res_pos = res[1]\n",
    "    conf = res[2]\n",
    "    topnn = res[3]\n",
    "    coverages_compo_pos_full.append(\n",
    "        {\n",
    "            \"confidence\": conf,\n",
    "            \"top_n\": topnn,\n",
    "            \"mean\": res_pos,\n",
    "        }\n",
    "    )\n",
    "    coverages_compo_int_full.append(\n",
    "        {\n",
    "            \"confidence\": conf,\n",
    "            \"top_n\": topnn,\n",
    "            \"mean\": res_int,\n",
    "        }\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a089af6d-f1e0-40e4-8794-125afa00d8aa",
   "metadata": {},
   "source": [
    "with open(\"coverages_compo_pos_full.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(\n",
    "        coverages_compo_pos_full,\n",
    "        handle,\n",
    "        protocol=pickle.HIGHEST_PROTOCOL,\n",
    "    )\n",
    "with open(\"coverages_compo_int_full.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(\n",
    "        coverages_compo_int_full,\n",
    "        handle,\n",
    "        protocol=pickle.HIGHEST_PROTOCOL,\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fb1c72b0-41e9-44b8-b6ab-cfe122d798af",
   "metadata": {},
   "source": [
    "with open(\"coverages_compo_pos_full.pickle\", \"rb\") as handle:\n",
    "    coverages_compo_pos_full = pickle.load(handle)\n",
    "with open(\"coverages_compo_int_full.pickle\", \"rb\") as handle:\n",
    "    coverages_compo_int_full = pickle.load(handle)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2e1a8e82-5fa6-4780-9263-2d1195ea2b16",
   "metadata": {},
   "source": [
    "n_coverages_compo_pos_full = []\n",
    "for idx in range(len(coverages_compo_pos_full)):\n",
    "    if not coverages_compo_pos_full[idx][\"mean\"]:\n",
    "        continue\n",
    "    coverages_compo_pos_full[idx][\"mean\"] = coverages_compo_pos_full[idx][\"mean\"][0]\n",
    "    n_coverages_compo_pos_full.append(coverages_compo_pos_full[idx])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "469260ea-7920-4109-a9d8-9005e6942c86",
   "metadata": {},
   "source": [
    "n_coverages_compo_int_full = []\n",
    "for idx in range(len(coverages_compo_int_full)):\n",
    "    if not coverages_compo_int_full[idx][\"mean\"]:\n",
    "        continue\n",
    "    coverages_compo_int_full[idx][\"mean\"] = coverages_compo_int_full[idx][\"mean\"][0]\n",
    "    n_coverages_compo_int_full.append(coverages_compo_int_full[idx])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "62db4a5e-e3d2-4a24-8032-950e41544bf3",
   "metadata": {},
   "source": [
    "df = pd.DataFrame(n_coverages_compo_pos_full)\n",
    "print(df.mean())\n",
    "(df.groupby([\"confidence\", \"top_n\"]).mean())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "524d485d-f25d-4144-956e-37f76cc9ee5b",
   "metadata": {},
   "source": [
    "df = pd.DataFrame(n_coverages_compo_int_full)\n",
    "print(df.mean())\n",
    "(df.groupby([\"confidence\", \"top_n\"]).mean())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Accuracy and Generalisation Plots",
   "id": "e3ccb33159147638"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot runs with one_hots\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "sns.boxplot(\n",
    "    data=runs_full_df[\n",
    "        runs_full_df.index.isin([\"DeGcChUp\", \"PaMMJSFQ\", \"PGszP2Fb\", \"HNEVXAXA\"])\n",
    "    ],\n",
    "    x=\"one_hot\",\n",
    "    y=\"max_acc_value\",\n",
    "    ax=ax,\n",
    ").set(\n",
    "    xlabel=\"Input type\",\n",
    "    ylabel=\"Max accuracy\",\n",
    "    xticklabels=[\n",
    "        \"Scalar\",\n",
    "        \"One Hot\",\n",
    "    ],\n",
    ")\n",
    "ax.yaxis.set_major_formatter(PercentFormatter(xmax=1))\n",
    "ax.set_ylim([0, 1])\n",
    "plt.savefig(\"one_hot.pdf\", bbox_inches=\"tight\", pad_inches=0)\n",
    "plt.savefig(\"one_hot.png\", bbox_inches=\"tight\", pad_inches=0)\n",
    "plt.show()"
   ],
   "id": "a44e75132690e150",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "not_one_hot_df = runs_full_df[runs_full_df[\"one_hot\"] == False]",
   "id": "e2d1165463a83596",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "sns.boxplot(\n",
    "    data=runs_full_df,\n",
    "    x=\"seq_length\",\n",
    "    y=\"max_acc_value\",\n",
    "    hue=\"sender_hidden\",\n",
    "    palette=palette,\n",
    ").set(\n",
    "    xlabel=\"Sequence Length\",\n",
    "    ylabel=\"Max accuracy\",\n",
    ")\n",
    "ax.yaxis.set_major_formatter(PercentFormatter(xmax=1))\n",
    "ax.set_ylim([0, 1])\n",
    "hatches = [\"///\", \"///\", \"///\", \"///\", \"..\"]\n",
    "# noinspection PyUnresolvedReferences\n",
    "patches = [patch for patch in ax.patches if type(patch) == mpl.patches.PathPatch]\n",
    "# iterate through the patches for each subplot\n",
    "for patch, hatch in zip(patches, hatches):\n",
    "    patch.set_hatch(hatch)\n",
    "    fc = patch.get_facecolor()\n",
    "    patch.set_edgecolor(fc)\n",
    "    patch.set_facecolor(\"none\")\n",
    "h, _ = ax.get_legend_handles_labels()\n",
    "l = ax.legend(\n",
    "    h,\n",
    "    [64, 128],\n",
    "    title=\"Hidden Size\",\n",
    "    ncols=2,\n",
    "    bbox_to_anchor=(0.45, 1.25),\n",
    "    loc=\"upper center\",\n",
    "    labelspacing=0.35,\n",
    "    columnspacing=1,\n",
    "    handletextpad=0.7,\n",
    ")\n",
    "for lp, hatch in zip(\n",
    "    l.get_patches(),\n",
    "    [\n",
    "        \"///\",\n",
    "        \"..\",\n",
    "    ],\n",
    "):\n",
    "    lp.set_hatch(hatch)\n",
    "    fc = lp.get_facecolor()\n",
    "    lp.set_edgecolor(fc)\n",
    "    lp.set_facecolor(\"none\")\n",
    "plt.savefig(\"seq_len.pdf\", bbox_inches=\"tight\", pad_inches=0)\n",
    "plt.savefig(\"seq_len.png\", bbox_inches=\"tight\", pad_inches=0)\n",
    "plt.show()"
   ],
   "id": "305fceab7955fbe5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "sns.boxplot(\n",
    "    data=runs_full_df[runs_full_df[\"seq_length\"] == 60],\n",
    "    x=\"vocab_size\",\n",
    "    y=\"max_acc_value\",\n",
    ").set(\n",
    "    xlabel=\"Vocabulary size\",\n",
    "    ylabel=\"Max accuracy\",\n",
    ")\n",
    "ax.yaxis.set_major_formatter(PercentFormatter(xmax=1, decimals=0))\n",
    "ax.set_ylim([0, 1])\n",
    "plt.savefig(\"vocab.pdf\", bbox_inches=\"tight\", pad_inches=0)\n",
    "plt.savefig(\"vocab.png\", bbox_inches=\"tight\", pad_inches=0)\n",
    "plt.show()"
   ],
   "id": "ceef3a18744fa155",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Evaluation of different sequence lengths\n",
    "\n",
    "The files required here are generated by eval_model.py"
   ],
   "id": "e12d2aff5c4516ab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open(\"data/eval_data.pickle\", \"rb\") as handle:\n",
    "    eval_data = pickle.load(handle)"
   ],
   "id": "34e0a7646b76bf0b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data_list = []\n",
    "\n",
    "for k in eval_data:\n",
    "    for seq in eval_data[k][\"BaseGRU\"]:\n",
    "        average = sum(eval_data[k][\"BaseGRU\"][seq]) / len(eval_data[k][\"BaseGRU\"][seq])\n",
    "        data_list.append(\n",
    "            {\n",
    "                \"train_seq_length\": not_one_hot_df[\"seq_length\"][k],\n",
    "                \"neg_seq\": seq,\n",
    "                \"average\": average,\n",
    "            }\n",
    "        )\n",
    "\n",
    "fig_data = pd.DataFrame(data_list)"
   ],
   "id": "482504c0712d1707",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "sns.boxplot(\n",
    "    data=fig_data,\n",
    "    x=\"neg_seq\",\n",
    "    y=\"average\",\n",
    "    hue=\"train_seq_length\",\n",
    "    palette=palette,\n",
    ").set(\n",
    "    xlabel=\"Seq. shortened by\",\n",
    "    ylabel=\"Avg. accuracy\",\n",
    ")\n",
    "hatches = sorted(\n",
    "    [\n",
    "        \"///\",\n",
    "        \"..\",\n",
    "        \"xx\",\n",
    "        \"OO\",\n",
    "        \"///\",\n",
    "        \"..\",\n",
    "        \"xx\",\n",
    "        \"OO\",\n",
    "        \"///\",\n",
    "        \"..\",\n",
    "        \"xx\",\n",
    "        \"OO\",\n",
    "        \"..\",\n",
    "        \"xx\",\n",
    "        \"OO\",\n",
    "        \"xx\",\n",
    "        \"OO\",\n",
    "    ]\n",
    ")\n",
    "# noinspection PyUnresolvedReferences\n",
    "patches = [patch for patch in ax.patches if type(patch) == mpl.patches.PathPatch]\n",
    "# iterate through the patches for each subplot\n",
    "for patch, hatch in zip(patches, hatches):\n",
    "    patch.set_hatch(hatch)\n",
    "    fc = patch.get_facecolor()\n",
    "    patch.set_edgecolor(fc)\n",
    "    patch.set_facecolor(\"none\")\n",
    "h, labels = ax.get_legend_handles_labels()\n",
    "l = ax.legend(\n",
    "    h,\n",
    "    labels,\n",
    "    title=\"Training Seq. Length\",\n",
    "    ncols=2,\n",
    "    bbox_to_anchor=(0.45, 1.35),\n",
    "    loc=\"upper center\",\n",
    "    labelspacing=0.35,\n",
    "    columnspacing=1,\n",
    "    handletextpad=0.7,\n",
    ")\n",
    "for lp, hatch in zip(l.get_patches(), sorted([\"///\", \"..\", \"xx\", \"OO\"])):\n",
    "    lp.set_hatch(hatch)\n",
    "    fc = lp.get_facecolor()\n",
    "    lp.set_edgecolor(fc)\n",
    "    lp.set_facecolor(\"none\")\n",
    "ax.yaxis.set_major_formatter(PercentFormatter(xmax=1, decimals=0))\n",
    "ax.set_ylim([0, 1])\n",
    "plt.savefig(\"eval.pdf\", bbox_inches=\"tight\", pad_inches=0)\n",
    "plt.savefig(\"eval.png\", bbox_inches=\"tight\", pad_inches=0)\n",
    "plt.show()"
   ],
   "id": "9f389b82f721dbb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "table = (\n",
    "    fig_data.groupby([\"train_seq_length\", \"neg_seq\"])\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .pivot_table(index=\"train_seq_length\", columns=\"neg_seq\", values=\"average\")\n",
    ")"
   ],
   "id": "b73956d771289f0d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "table[0] *= 100\n",
    "table[0] = table[0].round(2)\n",
    "columns = list(table.columns)\n",
    "columns.remove(0)\n",
    "for column in columns:\n",
    "    table[column] *= 100\n",
    "    table[column] -= table[0]\n",
    "    table[column] = table[column].round(2)"
   ],
   "id": "dadcc63154ab9033",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "table",
   "id": "3b05415d5e19493",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
