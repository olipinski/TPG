{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15c98257e9b77839",
   "metadata": {},
   "source": "# Analysis for synthetic languages"
  },
  {
   "cell_type": "code",
   "id": "330b7c2f0bec1c49",
   "metadata": {},
   "source": [
    "import copy\n",
    "import itertools\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm.notebook import tqdm\n",
    "from numpyencoder import NumpyEncoder\n",
    "\n",
    "# Workaround so we can re-use the project functions\n",
    "module_path = os.path.abspath(os.path.join(\"../\"))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from tpg.dataset import ProgressiveDataset\n",
    "from tpg.utils.npmi import (\n",
    "    compute_compositional_ngrams_positionals_npmi,\n",
    "    compute_compositional_ngrams_integers_npmi,\n",
    "    compute_non_compositional_npmi,\n",
    ")\n",
    "from tpg.utils.dict_utils import default_to_regular"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e1e2cd160f9ff782",
   "metadata": {},
   "source": [
    "sns.set(palette=\"pastel\")\n",
    "sns.set_style(\"whitegrid\")\n",
    "palette = sns.color_palette()\n",
    "\n",
    "import matplotlib.pylab as pylab\n",
    "\n",
    "params = {\n",
    "    \"legend.title_fontsize\": \"32\",\n",
    "    \"legend.fontsize\": \"24\",\n",
    "    \"axes.labelsize\": \"32\",\n",
    "    \"axes.titlesize\": \"32\",\n",
    "    \"xtick.labelsize\": \"22\",\n",
    "    \"ytick.labelsize\": \"26\",\n",
    "}\n",
    "pylab.rcParams.update(params)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8b167b7803fc1643",
   "metadata": {},
   "source": [
    "top_ns = [1, 2, 3, 5, 10, 15]\n",
    "confidences = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1ebd9948cad33309",
   "metadata": {},
   "source": [
    "## Create Synthetic Langauge to test measures"
   ]
  },
  {
   "cell_type": "code",
   "id": "9db0e3fdea425a89",
   "metadata": {},
   "source": [
    "rng = np.random.default_rng(42)\n",
    "\n",
    "example_ds = ProgressiveDataset(\n",
    "    seed=42,\n",
    "    dataset_size=10000,\n",
    "    num_points=60,\n",
    "    num_distractors=4,\n",
    "    repeat_chance=0,\n",
    "    sequence_window=True,\n",
    "    sequence_window_size=2,\n",
    "    use_random=True,\n",
    "    generate_special=None,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "976e57354fde452d",
   "metadata": {},
   "source": "### Synthetic Compositional Position Variant"
  },
  {
   "cell_type": "code",
   "id": "3ddfdfe693a4cdb6",
   "metadata": {},
   "source": [
    "cut_inputs = []\n",
    "messages = []\n",
    "guesses = []\n",
    "target_ids = []\n",
    "\n",
    "# Create random mapping from 2 length ngrams to integers\n",
    "random_tuples_sp = rng.choice(\n",
    "    list(itertools.product([x for x in range(2, 26)], repeat=2)), size=60, replace=False\n",
    ")\n",
    "\n",
    "for data in example_ds:\n",
    "    target_ids.append(data[3])\n",
    "    guesses.append(data[3])\n",
    "    cut_inputs.append(data[1])\n",
    "\n",
    "    # Switch between which positional to use\n",
    "    # here the positionals are position variant\n",
    "    # We only use r1 and l1 as positions\n",
    "    if rng.random() < 0.5:\n",
    "        # For l1 we use 1 in pos 0\n",
    "        # Unless target id is first pos\n",
    "        # Then we use r1\n",
    "        if data[1][0] == -1:\n",
    "            int_r1 = data[1][1]\n",
    "            message = np.concatenate((random_tuples_sp[int_r1], [1]))\n",
    "        else:\n",
    "            # Get the index of -1\n",
    "            idx_n1 = np.where(data[1] == -1)[0][0]\n",
    "            # Find integer to the left of the target\n",
    "            int_l1 = data[1][idx_n1 - 1]\n",
    "            message = np.concatenate(([1], random_tuples_sp[int_l1]))\n",
    "    else:\n",
    "        # For r1 we use 1 in pos 1\n",
    "        # Unless target id is last pos\n",
    "        # Then we use l1\n",
    "        if data[1][4] == -1:\n",
    "            int_l1 = data[1][3]\n",
    "            message = np.concatenate(([1], random_tuples_sp[int_l1]))\n",
    "        else:\n",
    "            # Get the index of -1\n",
    "            idx_n1 = np.where(data[1] == -1)[0][0]\n",
    "            # Find integer to the right of the target\n",
    "            int_r1 = data[1][idx_n1 + 1]\n",
    "            message = np.concatenate((random_tuples_sp[int_r1], [1]))\n",
    "\n",
    "    messages.append(message)\n",
    "\n",
    "\n",
    "exchange_dict = {}\n",
    "for y in range(len(example_ds)):\n",
    "    exchange_dict[f\"exchange_{y}\"] = {\n",
    "        \"cut_inputs\": cut_inputs[y],\n",
    "        \"message\": messages[y],\n",
    "        \"guess\": guesses[y],\n",
    "        \"target_id\": target_ids[y],\n",
    "    }\n",
    "\n",
    "with open(\n",
    "    os.path.join(\"./data/synthetic-logs/\", \"run-test-synthetic_pos-interactions.json\"),\n",
    "    \"w\",\n",
    ") as f:\n",
    "    json.dump(\n",
    "        exchange_dict,\n",
    "        f,\n",
    "        cls=NumpyEncoder,\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "849251c2ce69cbc8",
   "metadata": {},
   "source": "### Synthetic Compositional Position Invariant"
  },
  {
   "cell_type": "code",
   "id": "3c6df8f8f13470ba",
   "metadata": {},
   "source": [
    "cut_inputs = []\n",
    "messages = []\n",
    "guesses = []\n",
    "target_ids = []\n",
    "\n",
    "# Create random mapping from 2 length ngrams to integers\n",
    "random_tuples_spi = rng.choice(\n",
    "    list(itertools.product([x for x in range(3, 26)], repeat=2)), size=60, replace=False\n",
    ")\n",
    "\n",
    "for data in example_ds:\n",
    "    target_ids.append(data[3])\n",
    "    guesses.append(data[3])\n",
    "    cut_inputs.append(data[1])\n",
    "\n",
    "    # Switch between which positional to use\n",
    "    # here the positionals are position invariant\n",
    "    # We only use r1 and l1 as positions\n",
    "    if rng.random() < 0.5:\n",
    "        # For l1 we use 1 in any position\n",
    "        # Unless target id is first pos\n",
    "        # Then we use r1 - 2 in any position\n",
    "        if data[1][0] == -1:\n",
    "            int_r1 = data[1][1]\n",
    "            if rng.random() < 0.5:\n",
    "                message = np.concatenate((random_tuples_spi[int_r1], [2]))\n",
    "            else:\n",
    "                message = np.concatenate(([2], random_tuples_spi[int_r1]))\n",
    "        else:\n",
    "            # Get the index of -1\n",
    "            idx_n1 = np.where(data[1] == -1)[0][0]\n",
    "            # Find integer to the left of the target\n",
    "            int_l1 = data[1][idx_n1 - 1]\n",
    "            if rng.random() < 0.5:\n",
    "                message = np.concatenate((random_tuples_spi[int_l1], [1]))\n",
    "            else:\n",
    "                message = np.concatenate(([1], random_tuples_spi[int_l1]))\n",
    "    else:\n",
    "        # For r1 we use 1 in pos 1\n",
    "        # Unless target id is last pos\n",
    "        # Then we use l1\n",
    "        if data[1][4] == -1:\n",
    "            int_l1 = data[1][3]\n",
    "            if rng.random() < 0.5:\n",
    "                message = np.concatenate((random_tuples_spi[int_l1], [1]))\n",
    "            else:\n",
    "                message = np.concatenate(([1], random_tuples_spi[int_l1]))\n",
    "        else:\n",
    "            # Get the index of -1\n",
    "            idx_n1 = np.where(data[1] == -1)[0][0]\n",
    "            # Find integer to the right of the target\n",
    "            int_r1 = data[1][idx_n1 + 1]\n",
    "            if rng.random() < 0.5:\n",
    "                message = np.concatenate((random_tuples_spi[int_r1], [2]))\n",
    "            else:\n",
    "                message = np.concatenate(([2], random_tuples_spi[int_r1]))\n",
    "\n",
    "    messages.append(message)\n",
    "\n",
    "\n",
    "exchange_dict = {}\n",
    "for y in range(len(example_ds)):\n",
    "    exchange_dict[f\"exchange_{y}\"] = {\n",
    "        \"cut_inputs\": cut_inputs[y],\n",
    "        \"message\": messages[y],\n",
    "        \"guess\": guesses[y],\n",
    "        \"target_id\": target_ids[y],\n",
    "    }\n",
    "\n",
    "\n",
    "with open(\n",
    "    os.path.join(\n",
    "        \"./data/synthetic-logs/\", \"run-test-synthetic_posinv-interactions.json\"\n",
    "    ),\n",
    "    \"w\",\n",
    ") as f:\n",
    "    json.dump(\n",
    "        exchange_dict,\n",
    "        f,\n",
    "        cls=NumpyEncoder,\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "76be0850bc100086",
   "metadata": {},
   "source": "### Synthetic Compositional w Specials"
  },
  {
   "cell_type": "code",
   "id": "bc80fa11db9d4d23",
   "metadata": {},
   "source": [
    "cut_inputs = []\n",
    "messages = []\n",
    "guesses = []\n",
    "target_ids = []\n",
    "\n",
    "# Create random mapping from 2 length ngrams to integers\n",
    "random_tuples_spp = rng.choice(\n",
    "    list(itertools.product([x for x in range(4, 26)], repeat=2)), size=60, replace=False\n",
    ")\n",
    "specials = [[0, 0, 0], [1, 1, 1], [-1, -1, -1], [2, 2, 2], [3, 3, 3]]\n",
    "\n",
    "for data in example_ds:\n",
    "    target_ids.append(data[3])\n",
    "    guesses.append(data[3])\n",
    "    cut_inputs.append(data[1])\n",
    "\n",
    "    # Switch between which positional to use\n",
    "    # here the positionals are position invariant\n",
    "    # We only use l1 as position\n",
    "    # But we also have specials for begin, begin+1, end etc\n",
    "    # For l1 we use 4 in any position\n",
    "    # Unless target id is first pos\n",
    "    # Then we use r1 - 2 in any position\n",
    "\n",
    "    if data[1][2] == -1:\n",
    "        # Find integer to the left of the target\n",
    "        int_l1 = data[1][1]\n",
    "        if rng.random() < 0.5:\n",
    "            message = np.concatenate((random_tuples_spp[int_l1], [4]))\n",
    "        else:\n",
    "            message = np.concatenate(([4], random_tuples_spp[int_l1]))\n",
    "    else:\n",
    "        idx = np.where(data[1] == -1)[0][0]\n",
    "        message = copy.deepcopy(specials[idx])\n",
    "\n",
    "    messages.append(message)\n",
    "\n",
    "exchange_dict = {}\n",
    "for y in range(len(example_ds)):\n",
    "    exchange_dict[f\"exchange_{y}\"] = {\n",
    "        \"cut_inputs\": cut_inputs[y],\n",
    "        \"message\": messages[y],\n",
    "        \"guess\": guesses[y],\n",
    "        \"target_id\": target_ids[y],\n",
    "    }\n",
    "with open(\n",
    "    os.path.join(\n",
    "        \"./data/synthetic-logs/\", \"run-test-synthetic_pos_spec-interactions.json\"\n",
    "    ),\n",
    "    \"w\",\n",
    ") as f:\n",
    "    json.dump(\n",
    "        exchange_dict,\n",
    "        f,\n",
    "        cls=NumpyEncoder,\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Synthetic Non-Compositional with Specials",
   "id": "5269f1af1c8cfeae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cut_inputs = []\n",
    "messages = []\n",
    "guesses = []\n",
    "target_ids = []\n",
    "\n",
    "# Create random mapping from 2 length ngrams to integers\n",
    "random_tuples_nc = rng.choice(\n",
    "    list(itertools.product([x for x in range(4, 26)], repeat=3)), size=60, replace=False\n",
    ")\n",
    "specials = [[0, 0, 0], [1, 1, 1], [-1, -1, -1], [2, 2, 2], [3, 3, 3]]\n",
    "\n",
    "for data in example_ds:\n",
    "    target_ids.append(data[3])\n",
    "    guesses.append(data[3])\n",
    "    cut_inputs.append(data[1])\n",
    "\n",
    "    # Switch between which positional to use\n",
    "    # here the positionals are position invariant\n",
    "    # We only use l1 as position\n",
    "    # But we also have specials for begin, begin+1, end etc\n",
    "    # For l1 we use 4 in any position\n",
    "    # Unless target id is first pos\n",
    "    # Then we use r1 - 2 in any position\n",
    "\n",
    "    if data[1][2] == -1:\n",
    "        # Find integer to the left of the target\n",
    "        int_l1 = data[1][1]\n",
    "        message = copy.deepcopy(random_tuples_nc[int_l1])\n",
    "    else:\n",
    "        idx = np.where(data[1] == -1)[0][0]\n",
    "        message = copy.deepcopy(specials[idx])\n",
    "\n",
    "    messages.append(message)\n",
    "\n",
    "exchange_dict = {}\n",
    "for y in range(len(example_ds)):\n",
    "    exchange_dict[f\"exchange_{y}\"] = {\n",
    "        \"cut_inputs\": cut_inputs[y],\n",
    "        \"message\": messages[y],\n",
    "        \"guess\": guesses[y],\n",
    "        \"target_id\": target_ids[y],\n",
    "    }\n",
    "with open(\n",
    "    os.path.join(\n",
    "        \"./data/synthetic-logs/\", \"run-test-synthetic_nc_spec-interactions.json\"\n",
    "    ),\n",
    "    \"w\",\n",
    ") as f:\n",
    "    json.dump(\n",
    "        exchange_dict,\n",
    "        f,\n",
    "        cls=NumpyEncoder,\n",
    "    )"
   ],
   "id": "60c3567c4520dd49",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7db321bddd7b39a2",
   "metadata": {},
   "source": "## Load synthetic languages"
  },
  {
   "cell_type": "code",
   "id": "4b5bb74f24b7bc17",
   "metadata": {},
   "source": [
    "all_files = glob.glob(os.path.join(\"./data/synthetic-logs/\", \"*.json\"))\n",
    "li = []\n",
    "params = []\n",
    "for filename in tqdm(all_files):\n",
    "    split = filename.split(\"-\")\n",
    "    run_id = split[2]\n",
    "    architecture = split[3]\n",
    "    params.append([run_id, architecture])\n",
    "    df = pd.read_json(filename, orient=\"index\")\n",
    "    for k in [\n",
    "        \"cut_inputs\",\n",
    "        \"message\",\n",
    "        \"guess\",\n",
    "        \"target_id\",\n",
    "    ]:\n",
    "        df[k] = df[k].apply(lambda x: np.array(x))\n",
    "    li.append(df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "371a3a2c897b9e33",
   "metadata": {},
   "source": [
    "matches = {\n",
    "    f\"match_{x}\": {\n",
    "        \"run_id\": params[x][0],\n",
    "        \"architecture\": params[x][1],\n",
    "    }\n",
    "    for x in range(len(li))\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5a7a46025c31357b",
   "metadata": {},
   "source": [
    "for idx, match in enumerate(matches):\n",
    "    for col in li[idx].columns:\n",
    "        arr = []\n",
    "        for x in li[idx][col]:\n",
    "            arr.append(x)\n",
    "        arr = np.array(arr)\n",
    "        matches[match][col] = arr\n",
    "\n",
    "del li"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b893291fc7a2b4f6",
   "metadata": {},
   "source": [
    "for match in tqdm(matches):\n",
    "    guesses = matches[match][\"guess\"].flatten()\n",
    "    targets = matches[match][\"target_id\"].flatten()\n",
    "    correct = sum(guesses == targets)\n",
    "    total = len(targets)\n",
    "    matches[match][\"test_len\"] = total\n",
    "    matches[match][\"test_acc\"] = correct / total"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fe82b9f8d6026fa7",
   "metadata": {},
   "source": [
    "def compute_trwd_stats(match_to_compute) -> (dict, dict):\n",
    "    tpg_dict = defaultdict(\n",
    "        lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(int)))\n",
    "    )\n",
    "    obs_counts_dict = {x: 0 for x in [\"begin\", \"begin+1\", \"end-1\", \"end\"]}\n",
    "    for x in range(match_to_compute[\"test_len\"]):\n",
    "        if \"total\" not in tpg_dict[f'{match_to_compute[\"message\"][x]}']:\n",
    "            tpg_dict[f'{match_to_compute[\"message\"][x]}'][\"total\"] = 0\n",
    "        if \"correct\" not in tpg_dict[f'{match_to_compute[\"message\"][x]}']:\n",
    "            tpg_dict[f'{match_to_compute[\"message\"][x]}'][\"correct\"] = 0\n",
    "        if \"indices\" not in tpg_dict[f'{match_to_compute[\"message\"][x]}']:\n",
    "            tpg_dict[f'{match_to_compute[\"message\"][x]}'][\"indices\"] = []\n",
    "\n",
    "        tpg_dict[f'{match_to_compute[\"message\"][x]}'][\"total\"] += 1\n",
    "        if match_to_compute[\"target_id\"][x][0] == match_to_compute[\"guess\"][x]:\n",
    "            tpg_dict[f'{match_to_compute[\"message\"][x]}'][\"correct\"] += 1\n",
    "\n",
    "        s_obs = match_to_compute[\"cut_inputs\"][x]\n",
    "        if s_obs[0] == -1:\n",
    "            tpg_dict[f'{match_to_compute[\"message\"][x]}'][\"obs_neighbours\"][\"r1\"][\n",
    "                f\"{s_obs[1]}\"\n",
    "            ] += 1\n",
    "            tpg_dict[f'{match_to_compute[\"message\"][x]}'][\"obs_neighbours\"][\"r2\"][\n",
    "                f\"{s_obs[2]}\"\n",
    "            ] += 1\n",
    "            tpg_dict[f'{match_to_compute[\"message\"][x]}'][\"obs_neighbours\"][\"r3\"][\n",
    "                f\"{s_obs[3]}\"\n",
    "            ] += 1\n",
    "            tpg_dict[f'{match_to_compute[\"message\"][x]}'][\"obs_neighbours\"][\"r4\"][\n",
    "                f\"{s_obs[4]}\"\n",
    "            ] += 1\n",
    "\n",
    "            if \"begin\" not in tpg_dict[f'{match_to_compute[\"message\"][x]}']:\n",
    "                tpg_dict[f'{match_to_compute[\"message\"][x]}'][\"begin\"] = 0\n",
    "            tpg_dict[f'{match_to_compute[\"message\"][x]}'][\"begin\"] += 1\n",
    "            obs_counts_dict[\"begin\"] += 1\n",
    "\n",
    "        elif s_obs[1] == -1:\n",
    "            tpg_dict[f'{match_to_compute[\"message\"][x]}'][\"obs_neighbours\"][\"l1\"][\n",
    "                f\"{s_obs[0]}\"\n",
    "            ] += 1\n",
    "            tpg_dict[f'{match_to_compute[\"message\"][x]}'][\"obs_neighbours\"][\"r1\"][\n",
    "                f\"{s_obs[2]}\"\n",
    "            ] += 1\n",
    "            tpg_dict[f'{match_to_compute[\"message\"][x]}'][\"obs_neighbours\"][\"r2\"][\n",
    "                f\"{s_obs[3]}\"\n",
    "            ] += 1\n",
    "            tpg_dict[f'{match_to_compute[\"message\"][x]}'][\"obs_neighbours\"][\"r3\"][\n",
    "                f\"{s_obs[4]}\"\n",
    "            ] += 1\n",
    "\n",
    "            if \"begin+1\" not in tpg_dict[f'{match_to_compute[\"message\"][x]}']:\n",
    "                tpg_dict[f'{match_to_compute[\"message\"][x]}'][\"begin+1\"] = 0\n",
    "            tpg_dict[f'{match_to_compute[\"message\"][x]}'][\"begin+1\"] += 1\n",
    "            obs_counts_dict[\"begin+1\"] += 1\n",
    "\n",
    "        elif s_obs[2] == -1:\n",
    "            tpg_dict[f'{match_to_compute[\"message\"][x]}'][\"obs_neighbours\"][\"l1\"][\n",
    "                f\"{s_obs[1]}\"\n",
    "            ] += 1\n",
    "            tpg_dict[f'{match_to_compute[\"message\"][x]}'][\"obs_neighbours\"][\"l2\"][\n",
    "                f\"{s_obs[0]}\"\n",
    "            ] += 1\n",
    "            tpg_dict[f'{match_to_compute[\"message\"][x]}'][\"obs_neighbours\"][\"r1\"][\n",
    "                f\"{s_obs[3]}\"\n",
    "            ] += 1\n",
    "            tpg_dict[f'{match_to_compute[\"message\"][x]}'][\"obs_neighbours\"][\"r2\"][\n",
    "                f\"{s_obs[4]}\"\n",
    "            ] += 1\n",
    "        elif s_obs[3] == -1:\n",
    "            tpg_dict[f'{match_to_compute[\"message\"][x]}'][\"obs_neighbours\"][\"l1\"][\n",
    "                f\"{s_obs[2]}\"\n",
    "            ] += 1\n",
    "            tpg_dict[f'{match_to_compute[\"message\"][x]}'][\"obs_neighbours\"][\"l2\"][\n",
    "                f\"{s_obs[1]}\"\n",
    "            ] += 1\n",
    "            tpg_dict[f'{match_to_compute[\"message\"][x]}'][\"obs_neighbours\"][\"l3\"][\n",
    "                f\"{s_obs[0]}\"\n",
    "            ] += 1\n",
    "            tpg_dict[f'{match_to_compute[\"message\"][x]}'][\"obs_neighbours\"][\"r1\"][\n",
    "                f\"{s_obs[4]}\"\n",
    "            ] += 1\n",
    "            if \"end-1\" not in tpg_dict[f'{match_to_compute[\"message\"][x]}']:\n",
    "                tpg_dict[f'{match_to_compute[\"message\"][x]}'][\"end-1\"] = 0\n",
    "            tpg_dict[f'{match_to_compute[\"message\"][x]}'][\"end-1\"] += 1\n",
    "            obs_counts_dict[\"end-1\"] += 1\n",
    "        elif s_obs[4] == -1:\n",
    "            tpg_dict[f'{match_to_compute[\"message\"][x]}'][\"obs_neighbours\"][\"l1\"][\n",
    "                f\"{s_obs[3]}\"\n",
    "            ] += 1\n",
    "            tpg_dict[f'{match_to_compute[\"message\"][x]}'][\"obs_neighbours\"][\"l2\"][\n",
    "                f\"{s_obs[2]}\"\n",
    "            ] += 1\n",
    "            tpg_dict[f'{match_to_compute[\"message\"][x]}'][\"obs_neighbours\"][\"l3\"][\n",
    "                f\"{s_obs[1]}\"\n",
    "            ] += 1\n",
    "            tpg_dict[f'{match_to_compute[\"message\"][x]}'][\"obs_neighbours\"][\"l4\"][\n",
    "                f\"{s_obs[0]}\"\n",
    "            ] += 1\n",
    "            if \"end\" not in tpg_dict[f'{match_to_compute[\"message\"][x]}']:\n",
    "                tpg_dict[f'{match_to_compute[\"message\"][x]}'][\"end\"] = 0\n",
    "            tpg_dict[f'{match_to_compute[\"message\"][x]}'][\"end\"] += 1\n",
    "            obs_counts_dict[\"end\"] += 1\n",
    "\n",
    "    for key, v in tpg_dict.items():\n",
    "        values, counts = np.unique(\n",
    "            np.array(tpg_dict[key][\"indices\"]), return_counts=True\n",
    "        )\n",
    "        tpg_dict[key][\"indices_unq\"] = {\n",
    "            value: countt for value, countt in zip(values, counts)\n",
    "        }\n",
    "    return tpg_dict, obs_counts_dict"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4ab8d4e473c154bf",
   "metadata": {},
   "source": [
    "start_time = time.perf_counter()\n",
    "\n",
    "results = Parallel(n_jobs=4, verbose=10)(\n",
    "    delayed(compute_trwd_stats)(match_to_compute=matches[match]) for match in matches\n",
    ")\n",
    "\n",
    "for x in range(len(matches)):\n",
    "    matches[f\"match_{x}\"][\"tpg_stats\"] = copy.deepcopy(results[x][0])\n",
    "    matches[f\"match_{x}\"][\"obs_counts\"] = copy.deepcopy(results[x][1])\n",
    "\n",
    "finish_time = time.perf_counter()\n",
    "print(f\"Computing stats finished in {finish_time-start_time} seconds\")\n",
    "del results"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "86851cf624b841b9",
   "metadata": {},
   "source": [
    "# Calculate the normalised pointwise mutual information for non-compositional messages\n",
    "# There is some divisions by nans, so we ignore this for this block\n",
    "np.seterr(divide=\"ignore\", invalid=\"ignore\")\n",
    "\n",
    "# Check for the top_n integer/pos combinations\n",
    "for top_n in top_ns:\n",
    "    for match in tqdm(matches):\n",
    "        non_compositional_npmi_dict = compute_non_compositional_npmi(\n",
    "            matches[match], top_n\n",
    "        )\n",
    "        matches[match][f\"nc_npmi_{top_n}\"] = non_compositional_npmi_dict\n",
    "\n",
    "np.seterr(divide=\"warn\", invalid=\"warn\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c7304ea2e26d8db9",
   "metadata": {},
   "source": [
    "# Generate all n-grams\n",
    "# We do it the easy way-ish by generating all messages with say starting 1 and checking them all\n",
    "# \"16 20\" in \"[1 16 20]\"\n",
    "n_grams = defaultdict(dict)\n",
    "for x in [1, 2, 3]:\n",
    "    for n_gram in list(itertools.product([x for x in range(26)], repeat=x)):\n",
    "        n_grams[n_gram][\"length\"] = x\n",
    "n_grams = {\n",
    "    str(n_gram)\n",
    "    .replace(\"(\", \"\")\n",
    "    .replace(\")\", \"\")\n",
    "    .replace(\",\", \"\"): n_grams[n_gram][\"length\"]\n",
    "    for n_gram in n_grams.keys()\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fcf4e3558cd35ca5",
   "metadata": {},
   "source": [
    "start_time = time.perf_counter()\n",
    "\n",
    "for top_n in top_ns:\n",
    "    results = Parallel(n_jobs=4, verbose=10)(\n",
    "        delayed(compute_compositional_ngrams_integers_npmi)(\n",
    "            match=matches[match], n_grams=n_grams, top_n=top_n\n",
    "        )\n",
    "        for match in matches\n",
    "    )\n",
    "\n",
    "    for x in range(len(matches)):\n",
    "        matches[f\"match_{x}\"][f\"ngram_npmi_integers_{top_n}\"] = copy.deepcopy(\n",
    "            results[x][0]\n",
    "        )\n",
    "        if \"ngrams_pruned\" not in matches[f\"match_{x}\"]:\n",
    "            matches[f\"match_{x}\"][\"ngrams_pruned\"] = copy.deepcopy(results[x][1])\n",
    "\n",
    "finish_time = time.perf_counter()\n",
    "print(f\"Computing stats finished in {finish_time-start_time} seconds\")\n",
    "del results"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a7e9d753bb0d9cca",
   "metadata": {},
   "source": [
    "start_time = time.perf_counter()\n",
    "\n",
    "for confidence in confidences:\n",
    "    for top_n in top_ns:\n",
    "        results = Parallel(n_jobs=1, verbose=10)(\n",
    "            delayed(compute_compositional_ngrams_positionals_npmi)(\n",
    "                match=matches[match],\n",
    "                n_grams=n_grams,\n",
    "                confidence=confidence,\n",
    "                top_n=top_n,\n",
    "                scale=10,\n",
    "            )\n",
    "            for match in matches\n",
    "        )\n",
    "\n",
    "        for x in range(len(matches)):\n",
    "            matches[f\"match_{x}\"][\n",
    "                f\"ngram_npmi_positionals_{top_n}_{confidence}\"\n",
    "            ] = copy.deepcopy(results[x])\n",
    "\n",
    "finish_time = time.perf_counter()\n",
    "print(f\"Computing metrics finished in {finish_time-start_time} seconds\")\n",
    "del results"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e33b9ed202c9494c",
   "metadata": {},
   "source": [
    "# Find all the messages that are non-compositional\n",
    "nc_dicts = {}\n",
    "\n",
    "for top_n in tqdm(top_ns):\n",
    "    for confidence in confidences:\n",
    "        non_compositional_message_translation_dict = {}\n",
    "        for match in matches:\n",
    "            non_compositional_identified = []\n",
    "            non_compositional_message_translation_dict[match] = {\n",
    "                \"arch\": matches[match][\"architecture\"],\n",
    "                \"run_id\": matches[match][\"run_id\"],\n",
    "                \"positional_messages\": {\n",
    "                    x: [] for x in [\"begin\", \"begin+1\", \"end-1\", \"end\"]\n",
    "                },\n",
    "                \"other_messages\": defaultdict(lambda: defaultdict(list)),\n",
    "            }\n",
    "            non_compositional_npmi_dict = matches[match][f\"nc_npmi_{top_n}\"]\n",
    "            for msg in non_compositional_npmi_dict:\n",
    "                for special in [\"begin\", \"begin+1\", \"end-1\", \"end\"]:\n",
    "                    if non_compositional_npmi_dict[msg][special] >= confidence:\n",
    "                        # print(f\"{msg} is {special} in {match}\")\n",
    "                        non_compositional_identified.append(msg)\n",
    "                        non_compositional_message_translation_dict[match][\n",
    "                            \"positional_messages\"\n",
    "                        ][special].append(\n",
    "                            np.fromstring(\n",
    "                                msg.replace(\"[\", \"\").replace(\"]\", \"\").strip(),\n",
    "                                sep=\" \",\n",
    "                                dtype=np.int8,\n",
    "                            )\n",
    "                        )\n",
    "                        matches[match][\"non_compositional_emerged\"] = 1\n",
    "                for pos in non_compositional_npmi_dict[msg]:\n",
    "                    if pos in [\"begin\", \"begin+1\", \"end-1\", \"end\"]:\n",
    "                        continue\n",
    "                    if non_compositional_npmi_dict[msg][pos][\"npmi\"] >= confidence:\n",
    "                        ints = [\n",
    "                            int(x)\n",
    "                            for x in non_compositional_npmi_dict[msg][pos][\"ints\"]\n",
    "                        ]\n",
    "                        # print(f\"{msg} is {pos} for {ints} in {match}\")\n",
    "                        for x in ints:\n",
    "                            non_compositional_message_translation_dict[match][\n",
    "                                \"other_messages\"\n",
    "                            ][pos][x].append(\n",
    "                                np.fromstring(\n",
    "                                    msg.replace(\"[\", \"\").replace(\"]\", \"\").strip(),\n",
    "                                    sep=\" \",\n",
    "                                    dtype=np.int8,\n",
    "                                )\n",
    "                            )\n",
    "\n",
    "            for msg in non_compositional_identified:\n",
    "                count = 0\n",
    "                msg_c = [\n",
    "                    x\n",
    "                    for x in msg.replace(\"[\", \"\").replace(\"]\", \"\").strip().split(\" \")\n",
    "                    if x\n",
    "                ]\n",
    "\n",
    "                if msg_c[0] == msg_c[1] == msg_c[2]:\n",
    "                    if len(msg_c[0]) == 1:\n",
    "                        msg_c[0] = msg_c[0].join(\n",
    "                            \" \"\n",
    "                        )  # Make sure 1 is present as 1 not as 11, for example\n",
    "                    for msg1 in matches[match][\"tpg_stats\"]:\n",
    "                        if msg_c[0] in msg1:\n",
    "                            count += 1\n",
    "                else:\n",
    "                    continue\n",
    "                if count <= 2:\n",
    "                    matches[match][\"non_compositional_reserved_emerged\"] = 1\n",
    "\n",
    "        nc_dicts[\n",
    "            f\"topn_{top_n}-confidence_{confidence}\"\n",
    "        ] = non_compositional_message_translation_dict"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a7352cd9d6e01be5",
   "metadata": {},
   "source": [
    "# Find all n-grams that may represent some integers\n",
    "c_dicts = {}\n",
    "for top_n in tqdm(top_ns):\n",
    "    for confidence in confidences:\n",
    "        compositional_message_translation_dict = {}\n",
    "        for match in matches:\n",
    "            compositional_message_translation_dict[match] = {\n",
    "                \"arch\": matches[match][\"architecture\"],\n",
    "                \"run_id\": matches[match][\"run_id\"],\n",
    "                \"positional_ngrams\": defaultdict(\n",
    "                    lambda: defaultdict(list)\n",
    "                ),  # format is {requested_pos_reference: {needed_pos: [ngrams]}}\n",
    "                \"integer_ngrams\": defaultdict(\n",
    "                    lambda: defaultdict(list)\n",
    "                ),  # format is {requested_int_reference: {needed_pos: [ngrams]}}\n",
    "            }\n",
    "            ngram_npmi_integers_dict = matches[match][f\"ngram_npmi_integers_{top_n}\"]\n",
    "            ngram_npmi_positionals_dict = matches[match][\n",
    "                f\"ngram_npmi_positionals_{top_n}_{confidence}\"\n",
    "            ]\n",
    "\n",
    "            if len(ngram_npmi_positionals_dict.keys()) > 1:\n",
    "                matches[match][\"compositional_emerged\"] = 1\n",
    "\n",
    "            for ngram in ngram_npmi_integers_dict:\n",
    "                ngram_np = np.array([x for x in ngram.split(\" \") if x], dtype=np.uint8)\n",
    "                for pos in ngram_npmi_integers_dict[ngram]:\n",
    "                    if len(ngram_npmi_integers_dict[ngram][pos]) == 0:\n",
    "                        continue\n",
    "                    if ngram_npmi_integers_dict[ngram][pos][\"value\"] > confidence:\n",
    "                        for x in ngram_npmi_integers_dict[ngram][pos][\"integers\"]:\n",
    "                            compositional_message_translation_dict[match][\n",
    "                                \"integer_ngrams\"\n",
    "                            ][pos][int(x)].append(ngram_np)\n",
    "            for ngram in ngram_npmi_positionals_dict:\n",
    "                ngram_np = np.array([x for x in ngram.split(\" \")], dtype=np.uint8)\n",
    "                for pos in ngram_npmi_positionals_dict[ngram]:\n",
    "                    if len(ngram_npmi_positionals_dict[ngram][pos]) == 0:\n",
    "                        continue\n",
    "                    for referent_pos in ngram_npmi_positionals_dict[ngram][pos]:\n",
    "                        if (\n",
    "                            ngram_npmi_positionals_dict[ngram][pos][referent_pos]\n",
    "                            > confidence\n",
    "                        ):\n",
    "                            compositional_message_translation_dict[match][\n",
    "                                \"positional_ngrams\"\n",
    "                            ][pos][referent_pos].append(ngram_np)\n",
    "        c_dicts[\n",
    "            f\"topn_{top_n}-confidence_{confidence}\"\n",
    "        ] = compositional_message_translation_dict"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Save the data\n",
    "This data is used to debug, as it is very clean."
   ],
   "id": "761e8cc455f5f420"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "matches_dict = default_to_regular(matches)\n",
    "with open(\"matches_debug.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(matches_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ],
   "id": "ffbff69b35997d2a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "nc_dicts = default_to_regular(nc_dicts)\n",
    "with open(\"dictionary_nc_debug.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(\n",
    "        nc_dicts,\n",
    "        handle,\n",
    "        protocol=pickle.HIGHEST_PROTOCOL,\n",
    "    )"
   ],
   "id": "7174c60e4bcb2b46",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "c_dicts = default_to_regular(c_dicts)\n",
    "with open(\"dictionary_c_debug.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(\n",
    "        c_dicts,\n",
    "        handle,\n",
    "        protocol=pickle.HIGHEST_PROTOCOL,\n",
    "    )"
   ],
   "id": "b0076f0337c3d59a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
